{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d959b403-2be4-4c79-a31e-4e1317d54e7d",
   "metadata": {
    "id": "d959b403-2be4-4c79-a31e-4e1317d54e7d"
   },
   "source": [
    "# Wstęp do uczenia maszynowego - praca domowa nr 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4c156-1128-48ef-a085-0fb1c7046834",
   "metadata": {
    "id": "c6e4c156-1128-48ef-a085-0fb1c7046834"
   },
   "source": [
    "#### Jędrzej Sokołowski, Filip Szympliński\n",
    "#### 9 maja 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3613dd3-7411-4f07-99a0-74a6a041d5d6",
   "metadata": {
    "id": "b3613dd3-7411-4f07-99a0-74a6a041d5d6"
   },
   "source": [
    "### Wczytanie pakietów oraz danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e4a1e-b728-49c4-9679-15f155a3565d",
   "metadata": {
    "id": "326e4a1e-b728-49c4-9679-15f155a3565d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# ustawia domyślną wielkość wykresów\n",
    "plt.rcParams['figure.figsize'] = (8,8)\n",
    "# to samo tylko dla tekstu\n",
    "plt.rcParams['font.size'] = 16\n",
    "# ustawia wielkość tekstów dla wykresów seaborn zależną od wielkości wykresu\n",
    "sns.set_context('paper', font_scale=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50374f1-5e52-4e75-a948-61fa67baef5b",
   "metadata": {
    "id": "d50374f1-5e52-4e75-a948-61fa67baef5b"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/danieltytkowski/Downloads/urbanGB/urbanGB.txt\", sep=\",\", names=[\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6be909-9cc8-42ac-9c12-af8ecac153f8",
   "metadata": {
    "id": "ba6be909-9cc8-42ac-9c12-af8ecac153f8"
   },
   "source": [
    "### Szybkie spojrzenie na dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ba085-9cfe-4074-8fbd-0b4a93f5d235",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b0ba085-9cfe-4074-8fbd-0b4a93f5d235",
    "outputId": "3a9a303a-f78a-4ddf-9b7f-f92c949d66cb"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1655d-c537-48e9-b571-a83ca1ee999e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "cfa1655d-c537-48e9-b571-a83ca1ee999e",
    "outputId": "da12bac6-9b8f-4807-d8d7-0c427fbf5ecd"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "INCoTBM_zLat",
   "metadata": {
    "id": "INCoTBM_zLat"
   },
   "outputs": [],
   "source": [
    "data[\"x\"] = data[\"x\"]/1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-8bFxKKRzLh7",
   "metadata": {
    "id": "-8bFxKKRzLh7"
   },
   "source": [
    "Zgodnie z zaleceniem, które było w pliku README.md, skalujemu odpowiednio długość geograficzną punktów, aby ta zmienna lepiej odwzorywała odległości w stosunku do szerokości geograficznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300b757-762c-4c4d-9c0f-e24d9a4a1a36",
   "metadata": {
    "id": "e300b757-762c-4c4d-9c0f-e24d9a4a1a36"
   },
   "outputs": [],
   "source": [
    "X = data[\"x\"].values\n",
    "Y = data[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc7bda-54ed-4b28-ad56-fe4260ad2533",
   "metadata": {
    "id": "40cc7bda-54ed-4b28-ad56-fe4260ad2533"
   },
   "outputs": [],
   "source": [
    "data_array = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46362343-f7e8-4389-bb7e-8508f6a74c8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "46362343-f7e8-4389-bb7e-8508f6a74c8c",
    "outputId": "a8708361-e19f-4816-afdd-c1357c1993cb"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wr385W858bpm",
   "metadata": {
    "id": "wr385W858bpm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# Single output functions\n",
    "\n",
    "def min_interclust_dist(X, label):\n",
    "    clusters = set(label)\n",
    "    # X = X.to_numpy()\n",
    "    global_min_dist = np.inf\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        for cluster_j in clusters:\n",
    "            if cluster_i != cluster_j:\n",
    "                cluster_j_idx = np.where(label == cluster_j)\n",
    "                interclust_min_dist = np.min(distance.cdist(X[cluster_i_idx], X[cluster_j_idx]))\n",
    "                global_min_dist = np.min([global_min_dist, interclust_min_dist])\n",
    "    return global_min_dist\n",
    "\n",
    "def _inclust_mean_dists(X, label):\n",
    "    clusters = set(label)\n",
    "    # X = X.to_numpy()\n",
    "    inclust_dist_list = []\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        inclust_dist = np.mean(distance.pdist(X[cluster_i_idx]))\n",
    "        inclust_dist_list.append(inclust_dist)\n",
    "    return inclust_dist_list\n",
    "\n",
    "def mean_inclust_dist(X, label):\n",
    "    # X = X.to_numpy()\n",
    "    inclust_dist_list = _inclust_mean_dists(X, label)\n",
    "    return np.mean(inclust_dist_list)\n",
    "\n",
    "def std_dev_of_inclust_dist(X, label):\n",
    "    # X = X.to_numpy()\n",
    "    inclust_dist_list = _inclust_mean_dists(X, label)\n",
    "    return np.std(inclust_dist_list)\n",
    "\n",
    "def mean_dist_to_center(X, label):\n",
    "    clusters = set(label)\n",
    "    # X = X.to_numpy()\n",
    "    inclust_dist_list = []\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        cluster_i_mean = np.mean(X[cluster_i_idx], axis=0, keepdims=True)\n",
    "        inclust_dist = np.mean(distance.cdist(X[cluster_i_idx], cluster_i_mean))\n",
    "        inclust_dist_list.append(inclust_dist)\n",
    "    return np.mean(inclust_dist_list)\n",
    "\n",
    "\n",
    "metric_function_list = [mean_dist_to_center, std_dev_of_inclust_dist, mean_inclust_dist, min_interclust_dist]\n",
    "\n",
    "# Plot scores function\n",
    "def count_clustering_scores(X, cluster_num, model, score_fun):\n",
    "\n",
    "    if isinstance(cluster_num, int):\n",
    "        cluster_num_iter = [cluster_num]\n",
    "    else:\n",
    "        cluster_num_iter = cluster_num\n",
    "        \n",
    "    scores = []    \n",
    "    for k in cluster_num_iter:\n",
    "        model_instance = model(n_clusters=k)\n",
    "        labels = model_instance.fit_predict(X)\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)\n",
    "    \n",
    "    if isinstance(cluster_num, int):\n",
    "        return scores[0]\n",
    "    else:\n",
    "        return scores\n",
    "\n",
    "# All single output metrics function\n",
    "def print_metrics_results(X, model, n_cls, linkage_method):\n",
    "\n",
    "    model_instance = model(n_clusters=n_cls, linkage=linkage_method)\n",
    "    model_instance.fit(X)\n",
    "    y_pred= model_instance.predict(data)\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=y_pred, s=30, cmap='plasma')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Davies Bouldin Score: {davies_bouldin_score(X, y_pred)}\")\n",
    "    print(f\"Calinski Harabasz Score: {calinski_harabash_score(X, y_pred)}\")\n",
    "    print(f'Minimal distance between clusters = {count_clustering_scores(X, n_cls, model, min_interclust_dist):.2f}.')\n",
    "    print(f'Average distance between points in the same class = '\n",
    "      f'{count_clustering_scores(X, n_cls, model, mean_inclust_dist):.2f}.')\n",
    "    print(f'Standard deviation of distance between points in the same class = '\n",
    "      f'{count_clustering_scores(X, n_cls, model, std_dev_of_inclust_dist):.3f}.')\n",
    "    print(f'Average distance to cluster center = '\n",
    "      f'{count_clustering_scores(X, n_cls, model, mean_dist_to_center):.2f}.')\n",
    "\n",
    "\n",
    "# Plot functions\n",
    "\n",
    "def metric_score_results_for_models(X, label_list, score_fun):\n",
    "    scores = []    \n",
    "    for labels in label_list:\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)        \n",
    "    return scores\n",
    "\n",
    "\n",
    "def generate_model_labels(X, cluster_range, model, linkage_methods):\n",
    "    labels_list = []    \n",
    "    if model.__name__ == \"KMeans\":\n",
    "        for linkage in linkage_methods:\n",
    "            temp_list = []\n",
    "            for k in cluster_range:\n",
    "                model_instance = model(n_clusters=k, algorithm=linkage)\n",
    "                labels = model_instance.fit_predict(X)\n",
    "                temp_list.append(labels)\n",
    "            labels_list.append(temp_list)\n",
    "    elif model.__name__ == \"AgglomerativeClustering\":\n",
    "        for linkage in linkage_methods:\n",
    "            temp_list = []\n",
    "            for k in cluster_range:\n",
    "                model_instance = model(n_clusters=k, linkage=linkage)\n",
    "                labels = model_instance.fit_predict(X)\n",
    "                temp_list.append(labels)\n",
    "            labels_list.append(temp_list)\n",
    "    return labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637feb9-29fc-4f5a-9b4c-5a1e08a82692",
   "metadata": {
    "id": "b637feb9-29fc-4f5a-9b4c-5a1e08a82692"
   },
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wTH8Ffv7VXry",
   "metadata": {
    "id": "wTH8Ffv7VXry"
   },
   "source": [
    "Na początku spójrzmy jak są dzielone na róźne ilości klastrów nasze obserwacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43415b0-9f46-4c6f-aa6a-6d94c09e5c83",
   "metadata": {
    "id": "a43415b0-9f46-4c6f-aa6a-6d94c09e5c83"
   },
   "outputs": [],
   "source": [
    "def plot_kmeans_clusters(data, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(data)\n",
    "    y_kmeans = kmeans.predict(data)\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=y_kmeans, s=30, cmap='plasma')\n",
    "\n",
    "    centers = kmeans.cluster_centers_\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.75)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.title(f'K-means clusters, k={n_clusters}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VPMZM2a3zdIz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VPMZM2a3zdIz",
    "outputId": "dcc9e508-536c-49db-f640-8784bb334ecb"
   },
   "outputs": [],
   "source": [
    "k = 11\n",
    "\n",
    "fig, axes = plt.subplots(k,2, figsize = (5*2, 10*k))\n",
    "sdata = data.to_numpy()\n",
    "\n",
    "algorithms_list = k * ['full', 'elkan']\n",
    "n_clusters_list = [val for val in range(1,k+1,1) for _ in (0, 1)]\n",
    "\n",
    "for algorithm, ax, n_cl, n in zip(algorithms_list, axes.flatten(), n_clusters_list, range(2*k)):\n",
    "    model = KMeans(n_clusters=n_cl, algorithm=algorithms_list[n % 2])\n",
    "    model.fit(sdata)\n",
    "    y_predict = model.predict(sdata)\n",
    "    ax.scatter(sdata[:, 0], sdata[:, 1], c=y_predict, s=20, cmap='plasma')\n",
    "    centers = model.cluster_centers_\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.75)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title(f'Num of clusters: {n_cl}, algorithm: {algorithms_list[n % 2]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LizMADyQP6PL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LizMADyQP6PL",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1d03dfbd-4931-4607-f5c9-0f1201f447fd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "smaller_data = data.sample(frac=0.1).to_numpy()\n",
    "\n",
    "algorithms_list = ['full', 'elkan']\n",
    "cluster_range = [x for x in range(2, 12)]\n",
    "\n",
    "metric_function_list_doubled = [val for val in metric_function_list for _ in (0, 1)]\n",
    "k = len(metric_function_list_doubled)\n",
    "\n",
    "fig, axes = plt.subplots(k//2,2, figsize = (2*10, 10*k//2))\n",
    "\n",
    "label_list = generate_model_labels(smaller_data, cluster_range, KMeans, algorithms_list)\n",
    "for fun, ax, n in zip(metric_function_list_doubled, axes.flatten(), range(k)):\n",
    "    scores = metric_score_results_for_models(smaller_data, label_list[n%2], fun)\n",
    "    ax.plot(cluster_range, scores, 'bx-')\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel(f'{fun.__name__} score')\n",
    "    ax.set_title(f'KMeans with parameter (linkage/algorithm): {algorithms_list[n%2]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9o66HEmtbNF4",
   "metadata": {
    "id": "9o66HEmtbNF4"
   },
   "source": [
    "Jak widać odległości między punktami wewnątrz klastrów i między nimi naturalnie malała wraz ze wzrostem liczby podziałów. Co jednak ciekawe, dla k = 4, i algorytmu `full` mamy znacznie wyższą od reszty minimalną odległość między klastrami, co sugeruje, że wtedy są one najlepiej odseparowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c98b25-8182-444d-b6ed-c103947c6ce4",
   "metadata": {
    "id": "b3c98b25-8182-444d-b6ed-c103947c6ce4"
   },
   "outputs": [],
   "source": [
    "def count_wcss_scores(X, k_max, algorithm):\n",
    "    #  WCSS = within-cluster sum of squares\n",
    "    scores = []\n",
    "    for k in range(1, k_max+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0, algorithm=algorithm)\n",
    "        kmeans.fit(X)\n",
    "        wcss = kmeans.score(X) * -1 # score returns -WCSS\n",
    "        scores.append(wcss)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XjNWrUh60NJ9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "XjNWrUh60NJ9",
    "outputId": "ad66bcdd-51a2-4296-8676-500b6a80c8d9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (20, 10))\n",
    "wcss_vec = count_wcss_scores(data, 20, 'full')\n",
    "x_ticks = list(range(1, len(wcss_vec) + 1))\n",
    "ax[0].plot(x_ticks, wcss_vec, 'bx-')\n",
    "ax[0].set_xlabel('k')\n",
    "ax[0].set_ylabel('Within-cluster sum of squares')\n",
    "ax[0].set_title('The Elbow Method showing the optimal k for full argorithm option')\n",
    "\n",
    "wcss_vec = count_wcss_scores(data, 20, 'elkan')\n",
    "x_ticks = list(range(1, len(wcss_vec) + 1))\n",
    "ax[1].plot(x_ticks, wcss_vec, 'bx-')\n",
    "ax[1].set_xlabel('k')\n",
    "ax[1].set_ylabel('Within-cluster sum of squares')\n",
    "ax[1].set_title('The Elbow Method showing the optimal k for elkan argorithm option')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RoE3o3TNY2hd",
   "metadata": {
    "id": "RoE3o3TNY2hd"
   },
   "source": [
    "Z powyższych dwóch wykresów widać, że w przypadku obu metod (dwa różne algorytmy użyte przy implementacji) optymalna liczba klastrów wynosi około 5-6. Należy tutaj pamiętać, że odczytywanie takich informacji z tego typu wykresów nie stanowi ścisłego dowodu, a jest pewnego rodzaju heurystyką.\n",
    "\n",
    "Łącząc wnioski w poprzednich wykresów można wysunąć wnosek, że optymalna liczba klastrów wynosi bliżej 5. Jednocześnie warto zwrócić uwagę, że ta liczba może by także inna, bo w zależności od tego jak chcemy dzielić obserwacje, będziemy wykożystywać inne metryki. Wtedy to już nie ma gwarancji, że wyciągnięte wnioski będą analogiczne.\n",
    "\n",
    "Mimo to, jeśli byłaby potrzeba podania kokretnej liczby podziałów, to odpowiedź brzmiałaby 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8641ed8-1331-4061-9141-7145e4e47ee9",
   "metadata": {
    "id": "c8641ed8-1331-4061-9141-7145e4e47ee9"
   },
   "source": [
    "### AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71589818-aece-4688-b90f-54d8ed476751",
   "metadata": {
    "id": "71589818-aece-4688-b90f-54d8ed476751"
   },
   "outputs": [],
   "source": [
    "def aglomerative_clusters_plot(X, n_clusters, ax):\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    y = model.fit_predict(X)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='plasma', ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f26a0a-7d19-4359-877d-c6e533861a5e",
   "metadata": {
    "id": "83f26a0a-7d19-4359-877d-c6e533861a5e"
   },
   "outputs": [],
   "source": [
    "smaller_data = data.sample(frac=0.1).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad97f6-55e0-4f58-ac9f-45dcbee39f84",
   "metadata": {
    "id": "0bad97f6-55e0-4f58-ac9f-45dcbee39f84"
   },
   "source": [
    "#### Linkage: Ward vs Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xJ0nllrvh1cA",
   "metadata": {
    "id": "xJ0nllrvh1cA"
   },
   "source": [
    "Na początku spójrzmy jak są dzielone na róźne ilości klastrów nasze obserwacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fea778-e7c3-4805-9efa-decb65c46513",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b2fea778-e7c3-4805-9efa-decb65c46513",
    "outputId": "10ead58f-5ad1-4a65-d59b-3903f280add5"
   },
   "outputs": [],
   "source": [
    "k = 6\n",
    "\n",
    "fig, axes = plt.subplots(k,2, figsize = (5*2, 10*k))\n",
    "\n",
    "\n",
    "cluster_list_prep = [x for x in range(2,2*k+2,2) ]\n",
    "cluster_list_prep = [*cluster_list_prep[:2], 5, *cluster_list_prep[2:]] \n",
    "\n",
    "linkage_methods = k * ['ward', 'complete']\n",
    "n_clusters_list = [val for val in cluster_list_prep for _ in (0, 1)]\n",
    "\n",
    "for linkage, ax, n_cl, n in zip(linkage_methods, axes.flatten(), n_clusters_list, range(12)):\n",
    "    model = AgglomerativeClustering(n_clusters=n_cl, linkage=linkage_methods[n % 2])\n",
    "    y = model.fit_predict(smaller_data)\n",
    "    ax.scatter(smaller_data[:, 0], smaller_data[:, 1], c=y, s=30, cmap='plasma')\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title(f'Num of clusters: {n_cl}, linkage method: {linkage_methods[n % 2]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77dae1-8f32-40a8-b470-47b4450b7edc",
   "metadata": {
    "id": "de77dae1-8f32-40a8-b470-47b4450b7edc",
    "outputId": "6af89952-44e4-467b-c8ec-d0ab07704047"
   },
   "outputs": [],
   "source": [
    "# smaller_data = data.sample(frac=0.1).to_numpy()\n",
    "\n",
    "linkage_list = ['ward', 'complete']\n",
    "cluster_range = [x for x in range(2, 12, 2)]\n",
    "\n",
    "metric_function_list_doubled = [val for val in metric_function_list for _ in (0, 1)]\n",
    "k = len(metric_function_list_doubled)\n",
    "\n",
    "fig, axes = plt.subplots(k//2,2, figsize = (2*10, 10*k//2))\n",
    "\n",
    "label_list = generate_model_labels(smaller_data, cluster_range, AgglomerativeClustering, algorithms_list)\n",
    "for fun, ax, n in zip(metric_function_list_doubled, axes.flatten(), range(k)):\n",
    "    scores = metric_score_results_for_models(smaller_data, label_list[n%2], fun)\n",
    "    ax.plot(cluster_range, scores, 'bx-')\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel(f'{fun.__name__} score')\n",
    "    ax.set_title(f'AgglomerativeClustering with parameter (linkage/algorithm): {algorithms_list[n%2]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wzyARcPGrmUD",
   "metadata": {
    "id": "wzyARcPGrmUD"
   },
   "source": [
    "Porównując działanie algorytmu dla opcji `ward` i `complete` parametru linkage zachowują się dość podobnie, z wyjątkiem odchylenia standardowego między punktami w tym samym klastrze. Dla opcji `ward` jedynie dla 10 klastrów wartość ta się zmienia. Mówi nam to, że klastry są niezrównoważonych rozmiarów. W przypadku opcji `complete`, występuje tendencja do niezbalansowanych rozmiarów grup punktów, lecz wyjątkowo dla 4 klastrów, są one dobrze zbalansowane. Szczególnie widać to dobrze na mapach z zaznaczonymi klastrami."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6cb6a7-eaea-4c56-a366-3e7471225acb",
   "metadata": {
    "id": "4a6cb6a7-eaea-4c56-a366-3e7471225acb"
   },
   "source": [
    "#### Linkage: Average vs Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dyypHZ7HXv0B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dyypHZ7HXv0B",
    "outputId": "e38def7f-a74f-4080-f467-1319f7de2e71"
   },
   "outputs": [],
   "source": [
    "k=6\n",
    "\n",
    "fig, axes = plt.subplots(6,2, figsize = (5*2, 10*k))\n",
    "\n",
    "cluster_list_prep = [x for x in range(2,2*k+2,2) ]\n",
    "cluster_list_prep = [*cluster_list_prep[:2], 5, *cluster_list_prep[2:]] \n",
    "\n",
    "linkage_methods = k * ['average', 'single']\n",
    "n_clusters_list = [val for val in cluster_list_prep for _ in (0, 1)]\n",
    "\n",
    "for linkage, ax, n_cl, n in zip(linkage_methods, axes.flatten(), n_clusters_list, range(12)):\n",
    "    model = AgglomerativeClustering(n_clusters=n_cl, linkage=linkage_methods[n % 2])\n",
    "    y = model.fit_predict(smaller_data)\n",
    "    ax.scatter(smaller_data[:, 0], smaller_data[:, 1], c=y, s=30, cmap='plasma')\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title(f'Num of clusters: {n_cl}, linkage method: {linkage_methods[n % 2]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9PMaOWF14tJd",
   "metadata": {
    "id": "9PMaOWF14tJd"
   },
   "source": [
    "W przeciwieństwie do poprzenich metod łączenia klastrów, teraz widzimy wyraźne różnice w ich działaniu. \n",
    "Zastosowanie metody połączeń pojedyńczych, niesie za sobą to, że niezależnie od liczby klastrów, jeden z nich jest zawsze zdecydowanie większy od pozostałych.\n",
    "Możemy więc wnioskować, że ta metoda nie dzieli danych na klastry o porównywalnych licznościach.\n",
    "Metoda połączeń średnich działała w głównej mierze podobnie do wcześniejszych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e42b2f-db0e-4dcb-83bf-16612720cc93",
   "metadata": {
    "id": "48e42b2f-db0e-4dcb-83bf-16612720cc93",
    "outputId": "ec1e6792-69d5-4fff-fa0f-81e08de3ec23"
   },
   "outputs": [],
   "source": [
    "# smaller_data = data.sample(frac=0.1).to_numpy()\n",
    "\n",
    "linkage_list = ['average', 'single']\n",
    "cluster_range = [x for x in range(2, 12, 2)]\n",
    "\n",
    "metric_function_list_doubled = [val for val in metric_function_list for _ in (0, 1)]\n",
    "k = len(metric_function_list_doubled)\n",
    "\n",
    "fig, axes = plt.subplots(k//2,2, figsize = (2*10, 10*k//2))\n",
    "\n",
    "label_list = generate_model_labels(smaller_data, cluster_range, AgglomerativeClustering, linkage_list)\n",
    "for fun, ax, n in zip(metric_function_list_doubled, axes.flatten(), range(k)):\n",
    "    scores = metric_score_results_for_models(smaller_data, label_list[n%2], fun)\n",
    "    ax.plot(cluster_range, scores, 'bx-')\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel(f'{fun.__name__} score')\n",
    "    ax.set_title(f'AgglomerativeClustering with parameter (linkage/algorithm): {linkage_list[n%2]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QyQYGe0p6Xrd",
   "metadata": {
    "id": "QyQYGe0p6Xrd"
   },
   "source": [
    "Większość metryk, dla obu metod zachowywała się podobnie, otrzymywany wartości stopniowo malały wraz z wrostem liczby klastrów.\n",
    "Odchylenie standardowe wewnątrzklastrowe w przypadku metody połączeń średnich, z początku malało lecz ostatecznie zaczeło rosnąć, minimum zostało osiągnięte gdy liczba klastrów wynosiła 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8Aiwr0R7mao",
   "metadata": {
    "id": "e8Aiwr0R7mao"
   },
   "source": [
    "## Podsumowanie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4kieHjIh7r0o",
   "metadata": {
    "id": "4kieHjIh7r0o"
   },
   "source": [
    "Przy używaniu `KMeans`, z metody łokcia dla wyszło nam, że optymalną liczba klastrów jest 5. Gdy pracowaliśmy z `AgglomerativeClustering` dla rożnych metod łączenia wnioski były podobne. Wyniki dla wielu metryk malały wraz ze wzrostem klastrów, a niektóre z nich miały najmniejsze wartości w okolicach liczb 4-6. Tak więc widzimy pewne wskazania na liczbę 5.  \n",
    "\n",
    "Patrząc na wizualne podziały na klastry gdy było ich 5, łatwo znaleźć taki, który dokonuje podziału na regiony Wielkiej Brytanii.\n",
    "Przykładowo podział dokonany metodą aglomeracyjną, wykorzystujący połączenia średnie wyraźnie wyodrębnił region `South West` razem z częścią Walii, Londyn wraz z otaczającymi go regionami, centralną i północną część Wielkiej Brytanii oraz Szkocję.\n",
    "Gdy spojrzymy na mapę, taki podział jest zgodny z nasza intuicją, dlatego optymalną liczbą klastrów jest według nas 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-XKYAtDxdHVd",
   "metadata": {
    "id": "-XKYAtDxdHVd"
   },
   "source": [
    "## Walidacja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CGBqedRpdJDo",
   "metadata": {
    "id": "CGBqedRpdJDo"
   },
   "source": [
    "Walidujący: Daniel Tytkowski, Jan Skwarek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pBB-a9s_oUG_",
   "metadata": {
    "id": "pBB-a9s_oUG_"
   },
   "source": [
    "Sprawdzimy tylko co się stanie, jeśli przekształcimy współrzędne geograficzne na trójwymiarowe punkty na sferze i wtedy je sklastrujemy, używając SphericalKmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mv8Fz9Froy7t",
   "metadata": {
    "id": "mv8Fz9Froy7t"
   },
   "outputs": [],
   "source": [
    "def cartesian_encoder(coord, r_E=6371):  #6371\n",
    "    \"\"\"Convert lat/lon to cartesian points on Earth's surface.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "        coord : numpy 2darray (size=(N, 2))\n",
    "        r_E : radius of Earth\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "        out : numpy 2darray (size=(N, 3))\n",
    "    \"\"\"\n",
    "    lat , lon = np.deg2rad(coord[:,1]), np.deg2rad(coord[:,0])\n",
    "    x = r_E * np.cos(lat) * np.cos(lon)\n",
    "    y = r_E * np.sin(lon) * np.cos(lat)\n",
    "    z = r_E * np.sin(lat)\n",
    "    #return x, y, z\n",
    "\n",
    "    return np.concatenate([x.reshape(-1, 1), y.reshape(-1, 1), z.reshape(-1, 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b85f9",
   "metadata": {
    "id": "4c4b85f9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/danieltytkowski/Downloads/urbanGB/urbanGB.txt', header = None)\n",
    "df = df.sample(frac=0.3)\n",
    "df = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e03c06",
   "metadata": {
    "id": "85e03c06"
   },
   "outputs": [],
   "source": [
    "df_sphere = cartesian_encoder(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc96bd",
   "metadata": {
    "id": "d3cc96bd"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "def view3Dscatter(data,color=None):\n",
    "    fig = px.scatter_3d(x=data[:,0], y=data[:,1], z=data[:,2], color=color)\n",
    "    \n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae909d6a",
   "metadata": {
    "id": "ae909d6a",
    "outputId": "39116814-69a7-475a-ad0b-a728da47fac9"
   },
   "outputs": [],
   "source": [
    "view3Dscatter(df_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqWoAm75o4F3",
   "metadata": {
    "id": "kqWoAm75o4F3",
    "outputId": "c34593be-d870-420f-c1f6-e1a374452cc4"
   },
   "outputs": [],
   "source": [
    "! pip install coclust\n",
    "from coclust.clustering import SphericalKmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a403b",
   "metadata": {
    "id": "a78a403b"
   },
   "outputs": [],
   "source": [
    "sphereKmeans = KMeans(n_clusters=5)\n",
    "sphereKmeans2 = SphericalKmeans(n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2de32",
   "metadata": {
    "id": "cfb2de32"
   },
   "outputs": [],
   "source": [
    "cols = sphereKmeans.fit_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b316a",
   "metadata": {
    "id": "e49b316a",
    "outputId": "76b38552-3a57-4046-c601-9dd2503ebc01"
   },
   "outputs": [],
   "source": [
    "sphereKmeans.score(df_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3249220",
   "metadata": {
    "id": "e3249220"
   },
   "outputs": [],
   "source": [
    "colsClassicKMeansWithSpherical = sphereKmeans.fit_predict(df_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5a9bc",
   "metadata": {
    "id": "cde5a9bc",
    "outputId": "38e7b358-7577-4a00-bae7-b26b555e0910"
   },
   "outputs": [],
   "source": [
    "sphereKmeans2.fit(df_sphere)\n",
    "cols2=sphereKmeans2.row_labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c4b05",
   "metadata": {
    "id": "e65c4b05",
    "outputId": "457cef23-a18c-42f6-add6-4ce1dbca8621"
   },
   "outputs": [],
   "source": [
    "view3Dscatter(df_sphere,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506b7de",
   "metadata": {
    "id": "a506b7de",
    "outputId": "9d580be6-d7eb-4488-a13f-ac5cb9094b32"
   },
   "outputs": [],
   "source": [
    "view3Dscatter(df_sphere, cols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb570d",
   "metadata": {
    "id": "e4bb570d",
    "outputId": "5d8b27d6-53d6-4813-edfe-d0397af40099"
   },
   "outputs": [],
   "source": [
    "view3Dscatter(df_sphere, colsClassicKMeansWithSpherical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89260f04",
   "metadata": {
    "id": "89260f04"
   },
   "source": [
    "Podzial na klastry w 3 przypadkach: zwykły Kmeans+surowe wspolrzedne, spferyczny Kmeans+wspolrzednie trojwymiarowe i zwykly Kmeans+wspolrzedne trojwymiarowe roznia sie miedzy soba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ca28f",
   "metadata": {
    "id": "099ca28f"
   },
   "outputs": [],
   "source": [
    "def metrics_plots(max_k=10, X=None):\n",
    "\n",
    "    score = []\n",
    "    score_kmeans_s = []\n",
    "    score_kmeans_c = []\n",
    "    score_kmeans_d = []\n",
    "\n",
    "    for k in range(2, max_k):\n",
    "        kmeans = KMeans(n_clusters=k, random_state= 101)\n",
    "        predictions = kmeans.fit_predict(X)\n",
    "        # Calculate cluster validation metrics and append to lists of metrics\n",
    "        score.append(kmeans.score(X))\n",
    "        score_kmeans_s.append(silhouette_score(X, kmeans.labels_, metric='euclidean'))\n",
    "        score_kmeans_c.append(calinski_harabasz_score(X, kmeans.labels_))\n",
    "        score_kmeans_d.append(davies_bouldin_score(X, predictions))\n",
    "\n",
    "    list_scores = [score, score_kmeans_s, score_kmeans_c, score_kmeans_d] \n",
    "    # Elbow Method plot\n",
    "    list_title = ['Within-cluster sum of squares', 'Silhouette Score', 'Calinski Harabasz', 'Davies Bouldin'] \n",
    "    for i in range(len(list_scores)):\n",
    "        x_ticks = list(range(2, len(list_scores[i]) + 2))\n",
    "        plt.plot(x_ticks, list_scores[i], 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel(list_title[i])\n",
    "        plt.title('Optimal k')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052190c",
   "metadata": {
    "id": "b052190c",
    "outputId": "2844dd5c-4f62-4be0-9eae-aba630ac8162"
   },
   "outputs": [],
   "source": [
    "score = []\n",
    "for x in range(2,10):\n",
    "    model = KMeans(n_clusters=x)\n",
    "    model.fit(df_sphere)\n",
    "    score.append(model.score(df_sphere))\n",
    "x_ticks = list(range(2, 10))\n",
    "plt.plot(x_ticks, score, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Within-cluster sum of squares')\n",
    "plt.title('Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39528531",
   "metadata": {
    "id": "39528531"
   },
   "source": [
    "Podobnie jak wczesniej klastrow powinno byc 4 lub 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14131f",
   "metadata": {
    "id": "cf14131f"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc33a2",
   "metadata": {
    "id": "2ffc33a2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score \n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f5dab",
   "metadata": {
    "id": "b13f5dab"
   },
   "source": [
    "Sprawdzmy jak poradzi sobie DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed33929",
   "metadata": {
    "id": "aed33929"
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8742cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "minPts = 6\n",
    "nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=minPts).fit(df_sphere)\n",
    "distances, indices = nbrs.kneighbors(df_sphere)\n",
    "distanceDec = sorted(distances[:,minPts-1], reverse=True)\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "plt.xlabel('Indeks punktu po sortowaniu')\n",
    "plt.ylabel('Dystans od trzeciego najbliższego sąsiada')\n",
    "ax1.plot(list(range(1,df_sphere.shape[0]+1)), distanceDec)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2fb0b9",
   "metadata": {
    "id": "5c2fb0b9"
   },
   "outputs": [],
   "source": [
    "sphereDBCAN = DBSCAN(eps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc46bd",
   "metadata": {
    "id": "cbdc46bd"
   },
   "outputs": [],
   "source": [
    "dbCols = sphereDBCAN.fit_predict(df_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ed383",
   "metadata": {
    "id": "0e9ed383",
    "outputId": "57600d65-e3f9-4924-e7e4-052769b38fe1"
   },
   "outputs": [],
   "source": [
    "view3Dscatter(df_sphere, dbCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fAHr-WJldOCi",
   "metadata": {
    "id": "fAHr-WJldOCi"
   },
   "source": [
    "Praca wydaje się być w gruncie rzeczy bardzo dobra. Jedyny błąd to nie wzięcię pod uwagę, że punkty ze zbioru to punkty na sferze, a nie na płaszczyźnie (sprawdzimy czy robi to jakąś różnicę). Praca posiada podsumowanie, wiele wykresów, ciekawych wizualizacji. Metody zaproponowane przez zespół budujący są sprawdzane wieloma metrykami. Jest tylko kilka drobnych uwag, które postaramy się wypunktować:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jleQLgkYgATs",
   "metadata": {
    "id": "jleQLgkYgATs"
   },
   "source": [
    "1. Mało komentarzy - mogłoby się ich pojawić troszkę więcej. Szczególnie mowa tu o komentarzach przy funkcjach.\n",
    "2. Zdecydowana większość kodu skopiowana z laboratoryjnego pliku. Można było o tym wspomnieć.\n",
    "3. Brak informacji dla walidujących przed potencjalnie długo wykonującymi się funkjcami, czy też takimi, które bardzo obciążają RAM.\n",
    "4. Brak argumentacji dlaczego budujący decydują się akurat na taki wybór modeli lub chociażby informacji, że wybrane zostały one losowo.\n",
    "5. Brak informacji, która metoda klastreryzacji okazała się finalnie lepsza i dlaczego (samo wyciągnięcie wniosków odnośnie liczby klastrów zostało przeprowadzone dobrze).\n",
    "6. Metody są sprawdzane na 10% próbce, która jest inna dla każdej metody - nie jest to do końca błędne, ale można byłoby zrobić wizualizacje również dla tej samej próbki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jya4Z7p2jE_O",
   "metadata": {
    "id": "jya4Z7p2jE_O"
   },
   "source": [
    "Podsumowując, praca jest naprawdę bardzo solidnie wykonana. Nie wykryliśmy żadnych poważnych błędów. Analizy są wnikliwe, a wizualizacje czytelne. Widać, że budujący opanowali dobrze metody klasteryzacji i potrafią szukać skupień w tego typu zbiorach."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WalidHW5.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
